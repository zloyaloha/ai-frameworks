# ai-frameworks

### Классификация

Для задачи классификации наиболее стабильными и эффективными оказались модели на основе деревьев решений.
Решающее дерево показало лучшие результаты по F1 для минорного класса, которая является ключевой метрикой в условиях дисбаланса. Случайный лес также демонстрировал высокие результаты, особенно после предобработки, подтверждая способность ансамблей деревьев хорошо работать с разнородными и нелинейными признаками. Улучшенные версии KNN и линейных моделей (в том числе собственные реализации) показали себя значительно лучше базовых, однако всё же уступили решениям на деревьях, что логично для задач со сложной логикой признаков. В итоге, для классификации оптимальным выбором выглядит решающее дерево или случайный лес, так как они лучше других улавливают структуры в данных и устойчивы к дисбалансу.

|                            | ROC_AUC | F1 (1 класс) | F1 (2 класс) |
|----------------------------|---------|--------------|--------------|
| Baseline KNN               | 0.4674  | 0.81         | 0.06         |
| Improved KNN               | 0.8545  | 0.86         | 0.67         |
| My KNN                     | 0.8545  | 0.86         | 0.67         |
| Baseline Linear Classifier | 0.6118  | 0.86         | 0.00         |
| Linear Classifier          | 0.8513  | 0.88         | 0.62         |
| My Linear Classifier       | 0.8542  | 0.89         | 0.71         |
| Baseline Decision Tree     | 0.7447  | 0.86         | 0.61         |
| Decision Tree              | 0.8611  | 0.89         | 0.73         |
| My Decision Tree           | 0.8781  | 0.88         | 0.62         |
| Baseline Random Forest     | 0.8398  | 0.85         | 0.39         |
| Random Forest              | 0.8314  | 0.88         | 0.71         |
| My Random Forest           | 0.8515  | 0.88         | 0.59         |
| Base Random Forest         | 0.8380  | 0.88         | 0.65         |
| Random Forest              | 0.8497  | 0.88         | 0.67         |
| My Random Forest           | 0.8677  | 0.89         | 0.63         |

### Регрессия

Для задачи предсказания зарплаты НХЛ игроков неожиданно, но закономерно, лучшие результаты продемонстрировала линейная регрессия с предобработкой. Она показала наименьшую MAE и лучший R², что объясняется природой данных: во многом зарплаты объясняются суммарным влиянием множества факторов, близких к линейным. Деревья, случайные леса, градиентный бустинг работали хуже, вероятно из-за большого количества выбросов и высокой вариативности зарплат, что приводит деревья к переобучению. Бустинг при этом остаётся сильным быстрым бэйзлайном, если требуется более гибкая модель без тонкой подгонки. Таким образом, линейная регрессия оказалась наиболее подходящей моделью для регрессионной задачи, а бустинг - хорошим универсальным компромиссом.

|                        | MAE         | R2    |
|------------------------|-------------|-------|
| Base KNN               | 1173197.566 | 0.399 |
| KNN                    | 1027804.467 | 0.474 |
| My KNN                 | 1027804.467 | 0.474 |
| Base Linear Regression | 1381895.059 | 0.324 |
| Linear Regression      | 876739.311  | 0.644 |
| My Linear Regression   | 895660.583  | 0.620 |
| Base Decision Tree     | 1220967.395 | 0.322 |
| Decision Tree          | 938015.285  | 0.553 |
| My Decision Tree       | 963127.841  | 0.546 |
| Base Random Forest     | 973296.880  | 0.604 |
| Random Forest          | 928461.792  | 0.577 |
| My Random Forest       | 1104519.680 | 0.446 |
| Base Gradient Boosting | 963131.492  | 0.604 |
| Gradient Boosting      | 905427.990  | 0.560 |
| My gradient boosting   | 915546.804  | 0.569 |


### Вывод
- Для классификации: лучше всего подходят модели на деревьях (Decision Tree, Random Forest).

- Для регрессии: лидирует линейная регрессия; бустинг - сильный альтернативный вариант.

Собственные реализации моделей показали близкое качество к библиотечным версиям. Этот анализ демонстрирует, как важно учитывать структуру данных, их распределение, наличие выбросов и дисбаланс классов при выборе подходящего алгоритма. Не существует одной универсальной модели, подходящей для обеих задач, для обоих датасетов.